import os
# Embeddings
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain import HuggingFaceHub
from langchain.document_loaders import UnstructuredPDFLoader
from langchain.indexes import VectorstoreIndexCreator
import os
from langchain.chains import RetrievalQA
#from langchain.embedding import HuggingFaceEmbeddings
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.memory import ConversationSummaryBufferMemory
#from vectorhub.encoders.text import FlanT5Model


os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

embeddings = HuggingFaceEmbeddings()

db = FAISS.from_documents(docs, embeddings)

pdf_folder_path = '/kaggle/input/geog-const'
loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]

index = VectorstoreIndexCreator(
    embedding=HuggingFaceEmbeddings(),
    text_splitter=CharacterTextSplitter(chunk_size=950, chunk_overlap=0)).from_loaders(loaders)



# Create the language model
llm = HuggingFaceHub(repo_id="google/flan-t5-large", model_kwargs={"temperature": 1, "max_length": 512})



# Create the retrieval-based QA chain
chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=index.vectorstore.as_retriever(), input_key="question")



# Main loop
while True:
    # Get user input
    user_input = input("User: ")

    # Check if user wants to exit
    if user_input.lower() == "exit":
        print("Bot: Exiting...")
        break

    # Process user input with the retrieval-based QA chain
    response = chain.run(user_input)

    # Check if response is empty
    if not response:
        print("Bot: Data not found.")
        break

    # Print the response
    print("Bot:", response)





